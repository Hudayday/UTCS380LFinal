{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc515813-ceeb-4c42-98e2-e5484b6a4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================START==OF==EXPERIMENT=========================\n",
      "Model selected for experiment: GPT-J\n",
      "Device Mapinng selected for experiment: 1\n",
      "Is Cuda Availabe? : True\n",
      "Cuda Device Count : 2\n",
      "Full Name of Model selected for experiment: EleutherAI/gpt-j-6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tied parameters are on different devices: {'lm_head.weight': 'cpu', 'transformer.wte.weight': 1}. Please modify your custom device map or set `device_map='auto'`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Overview:\n",
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50400, 4096)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-27): 28 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Model Device Initial Mapping:\n",
      "{'transformer.wte': 0, 'transformer.drop': 0, 'transformer.h.0': 0, 'transformer.h.1': 0, 'transformer.h.2': 0, 'transformer.h.3': 0, 'transformer.h.4': 0, 'transformer.h.5': 0, 'transformer.h.6': 0, 'transformer.h.7': 0, 'transformer.h.8': 0, 'transformer.h.9': 0, 'transformer.h.10': 0, 'transformer.h.11': 0, 'transformer.h.12': 0, 'transformer.h.13': 0, 'transformer.h.14': 0, 'transformer.h.15': 0, 'transformer.h.16': 0, 'transformer.h.17.ln_1': 0, 'transformer.h.17.attn': 0, 'transformer.h.18': 1, 'transformer.h.19': 1, 'transformer.h.20': 1, 'transformer.h.21': 1, 'transformer.h.22': 1, 'transformer.h.23': 1, 'transformer.h.24': 1, 'transformer.h.25': 1, 'transformer.h.26': 1, 'transformer.h.27': 1, 'transformer.ln_f': 1, 'lm_head': 1, 'transformer.h.17.mlp': 1}\n",
      "\n",
      "======================START==OF==EVALUATION=========================\n",
      "======================START==GPT-J_1=========================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "transformer.h.14.ln_2.bias doesn't have any device set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 440\u001b[0m\n\u001b[1;32m    438\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_dual_seq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSELECTION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================START==OF==EVALUATION=========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 440\u001b[0m \u001b[43mmulti_evalute_multi_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSELECTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_disk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranformer_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m#multi_evalute(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type)\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================END==OF==EVALUATION=========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 408\u001b[0m, in \u001b[0;36mmulti_evalute_multi_model\u001b[0;34m(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type, ID)\u001b[0m\n\u001b[1;32m    406\u001b[0m t1 \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mmulti_evalute_multi_model_main, args\u001b[38;5;241m=\u001b[39m(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    407\u001b[0m t2 \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mmulti_evalute_multi_model_small, args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_small_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSELECTION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 408\u001b[0m \u001b[43mmulti_evalute_multi_model_small\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult_small_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSELECTION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSELECTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_disk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranformer_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m t1\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    410\u001b[0m t2\u001b[38;5;241m.\u001b[39mstart()\n",
      "Cell \u001b[0;32mIn[3], line 359\u001b[0m, in \u001b[0;36mmulti_evalute_multi_model_small\u001b[0;34m(output_filename, MODEL, SELECTION, model2, model_name, model_type, tranformer_type, ID)\u001b[0m\n\u001b[1;32m    356\u001b[0m pid \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpid\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================START==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 359\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEleutherAI/gpt-neo-1.3B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_auto_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_only_gpu_1_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mprint\u001b[39m(model2\u001b[38;5;241m.\u001b[39mhf_device_map)\n\u001b[1;32m    361\u001b[0m total, free, used \u001b[38;5;241m=\u001b[39m calc_GPU_info(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3492\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3498\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3870\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3870\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3877\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3878\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3885\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:733\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    730\u001b[0m         module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(module_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m device_map:\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;66;03m# TODO: group all errors and raise at the end.\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have any device set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    734\u001b[0m     param_device \u001b[38;5;241m=\u001b[39m device_map[module_name]\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: transformer.h.14.ln_2.bias doesn't have any device set."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import signal\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pynvml import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, Union, List, Tuple\n",
    "from transformers.utils.hub import convert_file_size_to_int\n",
    "from accelerate import infer_auto_device_map, init_empty_weights, disk_offload\n",
    "from transformers import pipeline, BitsAndBytesConfig, GPTNeoForCausalLM, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "\n",
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def get_precision_from_type(float_precision):\n",
    "    # need improvement\n",
    "    # Extract numbers using list comprehension and join them\n",
    "    return int(''.join([char for char in float_precision if char.isdigit()]))\n",
    "\n",
    "\n",
    "def calc_memeory_req(model, float_precision):\n",
    "    size_bytes = count_parameters(model) * get_precision_from_type(str(float_precision)) / 8\n",
    "    return bytes_to_giga_bytes(size_bytes)\n",
    "\n",
    "\n",
    "def calc_GPU_info(dev):\n",
    "    if torch.cuda.is_available():\n",
    "        nvmlInit()\n",
    "        h = nvmlDeviceGetHandleByIndex(dev)\n",
    "        info = nvmlDeviceGetMemoryInfo(h)\n",
    "        return info.total, info.free, info.used #return Bytes\n",
    "    else:\n",
    "      return 0, 0, 0\n",
    "    \n",
    "\n",
    "def flush():\n",
    "      gc.collect()\n",
    "      torch.cuda.empty_cache()\n",
    "      torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "def init_experiment(MODEL):\n",
    "    if MODEL == \"GPT-Neo\" : return \"EleutherAI/gpt-neo-2.7B\", GPTNeoForCausalLM, GPT2Tokenizer\n",
    "    elif MODEL == \"GPT-J\" : return \"EleutherAI/gpt-j-6B\", AutoModelForCausalLM, AutoTokenizer\n",
    "    elif MODEL == \"GPT-NeoX\" : return \"EleutherAI/gpt-neox-20b\", GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "\n",
    "# Rewrite from accelerate for pure GPU usage\n",
    "def get_max_memory(number_of_GPU_in_use = -1, max_memory: Optional[Dict[Union[int, str], Union[int, str]]] = None):\n",
    "    \"\"\"\n",
    "    Get the maximum memory available if nothing is passed, converts string to int otherwise.\n",
    "    number_of_GPU_in_use will ignore extra GPUs, -1 means no limitation\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "\n",
    "    if max_memory is None:\n",
    "        if not (torch.cuda.is_available()):\n",
    "            max_memory = {}\n",
    "\n",
    "        else:\n",
    "            if number_of_GPU_in_use == -1 or torch.cuda.device_count() < number_of_GPU_in_use:\n",
    "                num_GPU = torch.cuda.device_count()\n",
    "            else:\n",
    "                num_GPU = number_of_GPU_in_use\n",
    "            # Make sure CUDA is initialized on each GPU to have the right memory info.\n",
    "            for i in range(num_GPU):\n",
    "                _ = torch.tensor([0], device=i)\n",
    "            max_memory = {i: 0.9*torch.cuda.mem_get_info(i)[0] for i in range(num_GPU)}\n",
    "\n",
    "        max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "        return max_memory\n",
    "\n",
    "    for key in max_memory:\n",
    "        if isinstance(max_memory[key], str):\n",
    "            max_memory[key] = convert_file_size_to_int(max_memory[key])\n",
    "\n",
    "    # Need to sort the device by type to make sure that we allocate the gpu first.\n",
    "    # As gpu/xpu are represented by int, we need to sort them first.\n",
    "    gpu_devices = [k for k in max_memory.keys() if isinstance(k, int)]\n",
    "    gpu_devices.sort()\n",
    "    # check if gpu devices are available and if not, throw a warning\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for device in gpu_devices:\n",
    "        if device >= num_devices or device < 0:\n",
    "            print(f\"Device {device} is not available, available devices are {list(range(num_devices))}\")\n",
    "    # Add the other devices in the preset order if they are available\n",
    "    all_devices = gpu_devices + [k for k in [\"mps\", \"cpu\", \"disk\"] if k in max_memory.keys()]\n",
    "    # Raise an error if a device is not recognized\n",
    "    for k in max_memory.keys():\n",
    "        if k not in all_devices:\n",
    "            raise ValueError(\n",
    "                f\"Device {k} is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'\"\n",
    "            )\n",
    "    max_memory = {k: max_memory[k] for k in all_devices}\n",
    "\n",
    "    return max_memory\n",
    "\n",
    "def get_only_gpu_1_memory(number_of_GPU_in_use = -1, max_memory: Optional[Dict[Union[int, str], Union[int, str]]] = None):\n",
    "    \"\"\"\n",
    "    Only use GPU1\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "\n",
    "    if max_memory is None:\n",
    "        if not (torch.cuda.is_available()):\n",
    "            max_memory = {}\n",
    "\n",
    "        else:\n",
    "            if number_of_GPU_in_use == -1 or torch.cuda.device_count() < number_of_GPU_in_use:\n",
    "                num_GPU = torch.cuda.device_count()\n",
    "            else:\n",
    "                num_GPU = number_of_GPU_in_use\n",
    "            # Make sure CUDA is initialized on each GPU to have the right memory info.\n",
    "            for i in range(num_GPU):\n",
    "                _ = torch.tensor([0], device=i)\n",
    "            max_memory = {i: 0.9*torch.cuda.mem_get_info(i)[0] for i in range(1,num_GPU)}\n",
    "\n",
    "        max_memory[0] = 0\n",
    "        max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "        return max_memory\n",
    "\n",
    "    for key in max_memory:\n",
    "        if isinstance(max_memory[key], str):\n",
    "            max_memory[key] = convert_file_size_to_int(max_memory[key])\n",
    "\n",
    "    # Need to sort the device by type to make sure that we allocate the gpu first.\n",
    "    # As gpu/xpu are represented by int, we need to sort them first.\n",
    "    gpu_devices = [k for k in max_memory.keys() if isinstance(k, int)]\n",
    "    gpu_devices.sort()\n",
    "    # check if gpu devices are available and if not, throw a warning\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for device in gpu_devices:\n",
    "        if device >= num_devices or device < 0:\n",
    "            print(f\"Device {device} is not available, available devices are {list(range(num_devices))}\")\n",
    "    # Add the other devices in the preset order if they are available\n",
    "    all_devices = gpu_devices + [k for k in [\"mps\", \"cpu\", \"disk\"] if k in max_memory.keys()]\n",
    "    # Raise an error if a device is not recognized\n",
    "    for k in max_memory.keys():\n",
    "        if k not in all_devices:\n",
    "            raise ValueError(\n",
    "                f\"Device {k} is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'\"\n",
    "            )\n",
    "    max_memory = {k: max_memory[k] for k in all_devices}\n",
    "\n",
    "    return max_memory\n",
    "\n",
    "\n",
    "def model_loader(model_disk, model_name, selection : int = 0, model_type = GPTNeoXForCausalLM, float_precision = torch.bfloat16):\n",
    "    '''\n",
    "    Huday v 0.0.1\n",
    "    Para selection represents the way of loading the model,\n",
    "    which some of them are limited to the number of graphic card\n",
    "    0: out-of-the-box \"auto\" loadiing (\"auto\" or \"balanced\": Accelerate will split the weights so that each GPU is used equally)\n",
    "    1: out-of-the-box \"sequential\" loading (\"sequential\": Accelerate will fill the GPUs in order (so the last ones might not be used at all))\n",
    "    # 2: custom balanced loading\n",
    "    # 3: custom sequential loading\n",
    "    2: Quantization, force load on 1 GPU (manual now)\n",
    "    3: vanilla transformer force load on one GPU\n",
    "    '''\n",
    "    if selection == 0:\n",
    "        model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map='auto')\n",
    "    elif selection == 1:\n",
    "        model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map='sequential')\n",
    "    # elif selection == 2:\n",
    "    #     df = custom_balanced_loading(model_disk, float_precision)\n",
    "    #     model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map=df)\n",
    "    # elif selection == 3:\n",
    "    #     df = custom_sequential_loading(model_disk, float_precision)\n",
    "    #     model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map=df)\n",
    "    elif selection == 2: #INT8\n",
    "        model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map=infer_auto_device_map(model_disk,get_max_memory(1)),\\\n",
    "                                           quantization_config=BitsAndBytesConfig(load_in_8bit=True,\n",
    "                                                                                  bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "                                                                                  llm_int8_enable_fp32_cpu_offload=True # we need to add this foor cpu offloading\n",
    "                                                                                  ))\n",
    "    elif selection == 3:\n",
    "        model = model_type.from_pretrained(model_name, torch_dtype=float_precision, device_map=infer_auto_device_map(model_disk,get_max_memory(1)))\n",
    "    elif selection == 4:\n",
    "        model = model_type.from_pretrained(model_name, torch_dtype=float_precision)\n",
    "    elif selection == 5:\n",
    "        model = model_type.from_pretrained(model_name)\n",
    "    if selection < 4:\n",
    "        print(model.hf_device_map)\n",
    "    print(\"Total Memeory Requirement of {} is {:.2f} GB\".format(model_name, calc_memeory_req(model, float_precision)))\n",
    "    print(\"Maximum GPU Memory Usage of {} is {:.2f} GB\".format(model_name, bytes_to_giga_bytes(torch.cuda.max_memory_allocated())))\n",
    "    total, free, used = calc_GPU_info(0)\n",
    "    print(\"GPU-1 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        total, free, used = calc_GPU_info(1)\n",
    "        print(\"GPU-2 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(pipe, prompt):\n",
    "    in_bytes = sys.getsizeof(prompt) # size of input (bytes)\n",
    "    start = time.time() # start measutring time (sec)\n",
    "    conv = pipe(prompt) # call pipeline\n",
    "    end = time.time() # stop measurinng time (sec)\n",
    "    latency = end - start # (sec)\n",
    "    throughput = in_bytes/latency # (bytes/sec)\n",
    "    return conv, latency, throughput\n",
    "\n",
    "\n",
    "def multi_evalute(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type):\n",
    "    fields = ['Timestamp', 'Load Type', 'Prompt', 'output','Latency ave (sec)', 'Latency std (sec)', 'Throughput ave (bytes/sec)', 'Throughput std (bytes/sec)']\n",
    "    out_list = []\n",
    "    \n",
    "    i = SELECTION\n",
    "\n",
    "    command_record = [\n",
    "    \"nvidia-smi\",\n",
    "    \"--query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used\",\n",
    "    \"--format=csv\",\n",
    "    \"-l\", \"1\",\n",
    "    \"-f\", f\"./GPU-record-{MODEL}-method_{i}.csv\"\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command_record) \n",
    "    pid = process.pid\n",
    "    \n",
    "    print(f\"======================START=={MODEL}_{i}=========================\")\n",
    "    model = model_loader(model_disk, model_name, selection = i, model_type = model_type, float_precision = torch.bfloat16)\n",
    "    tokenizer = tranformer_type.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    filename = \"input_prompt.txt\"\n",
    "    with open(filename) as file:\n",
    "        for prompt in file:\n",
    "            conv_list = []\n",
    "            lat_list = []\n",
    "            thrput_list = []\n",
    "            for j in range(3):   #Take average and std\n",
    "                conv, lat, thrput = evaluate(pipe, prompt)\n",
    "                conv_list.append(conv)\n",
    "                lat_list.append(lat)\n",
    "                thrput_list.append(thrput)\n",
    "                \n",
    "            timestamp = time.time()\n",
    "\n",
    "            # Convert the timestamp to a datetime object\n",
    "            dt_object = datetime.fromtimestamp(timestamp)\n",
    "\n",
    "            # Format the datetime object\n",
    "            formatted_time = dt_object.strftime('%Y/%m/%d %H:%M:%S.%f')\n",
    "            \n",
    "            out_list.append([formatted_time,i,prompt,conv_list,np.mean(lat_list),np.std(lat_list),np.mean(thrput_list),np.std(thrput_list)])\n",
    "            \n",
    "        del model\n",
    "        flush()\n",
    "        os.kill(pid, signal.SIGINT)\n",
    "        print(f\"======================END=={MODEL}_{i}=========================\")\n",
    "\n",
    "    with open(output_filename, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        csvwriter.writerow(fields)   \n",
    "        csvwriter.writerows(out_list)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_evalute_multi_model_main(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID):\n",
    "    fields = ['Timestamp', 'Load Type', 'Prompt', 'output','Latency ave (sec)', 'Latency std (sec)', 'Throughput ave (bytes/sec)', 'Throughput std (bytes/sec)']\n",
    "    out_list = []\n",
    "    \n",
    "    i = SELECTION\n",
    "\n",
    "    command_record = [\n",
    "    \"nvidia-smi\",\n",
    "    \"--query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used\",\n",
    "    \"--format=csv\",\n",
    "    \"-l\", \"1\",\n",
    "    \"-f\", f\"./GPU-record-dual-main-seq-{MODEL}-method_{i}_{ID}.csv\"\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command_record) \n",
    "    pid = process.pid\n",
    "    \n",
    "    print(f\"======================START=={MODEL}_{i}=========================\")\n",
    "    model = model_loader(model_disk, model_name, selection = i, model_type = model_type, float_precision = torch.bfloat16)\n",
    "    total, free, used = calc_GPU_info(0)\n",
    "    print(\"GPU-1 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        total, free, used = calc_GPU_info(1)\n",
    "        print(\"GPU-2 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "    \n",
    "    tokenizer = tranformer_type.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    filename = \"input_prompt.txt\"\n",
    "    with open(filename) as file:\n",
    "        for prompt in file:\n",
    "            conv_list = []\n",
    "            lat_list = []\n",
    "            thrput_list = []\n",
    "\n",
    "            conv_list2 = []\n",
    "            lat_list2 = []\n",
    "            thrput_list2 = []\n",
    "            for j in range(3):   #Take average and std\n",
    "                conv, lat, thrput = evaluate(pipe, prompt)\n",
    "                conv_list.append(conv)\n",
    "                lat_list.append(lat)\n",
    "                thrput_list.append(thrput)\n",
    "\n",
    "            timestamp = time.time()\n",
    "\n",
    "            # Convert the timestamp to a datetime object\n",
    "            dt_object = datetime.fromtimestamp(timestamp)\n",
    "\n",
    "            # Format the datetime object\n",
    "            formatted_time = dt_object.strftime('%Y/%m/%d %H:%M:%S.%f')\n",
    "            \n",
    "            out_list.append([formatted_time,i,prompt,conv_list,np.mean(lat_list),np.std(lat_list),np.mean(thrput_list),np.std(thrput_list)])\n",
    "            \n",
    "        del model\n",
    "        flush()\n",
    "        os.kill(pid, signal.SIGINT)\n",
    "        print(f\"======================END=={MODEL}_{i}=========================\")\n",
    "\n",
    "    with open(output_filename, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        csvwriter.writerow(fields)   \n",
    "        csvwriter.writerows(out_list)    \n",
    "\n",
    "def multi_evalute_multi_model_small(output_filename, MODEL, SELECTION, model2, model_name, model_type, tranformer_type,ID):\n",
    "    fields = ['Timestamp', 'Load Type', 'Prompt', 'output','Latency ave (sec)', 'Latency std (sec)', 'Throughput ave (bytes/sec)', 'Throughput std (bytes/sec)']\n",
    "    out_list = []\n",
    "    \n",
    "    i = SELECTION\n",
    "\n",
    "    command_record = [\n",
    "    \"nvidia-smi\",\n",
    "    \"--query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used\",\n",
    "    \"--format=csv\",\n",
    "    \"-l\", \"1\",\n",
    "    \"-f\", f\"./GPU-record-dual-small-seq-{MODEL}-method_{i}_{ID}.csv\"\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command_record) \n",
    "    pid = process.pid\n",
    "    \n",
    "    print(f\"======================START=={MODEL}_{i}=========================\")\n",
    "    model2 = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.bfloat16, device_map=infer_auto_device_map(model2,get_only_gpu_1_memory()))\n",
    "    print(model2.hf_device_map)\n",
    "    total, free, used = calc_GPU_info(0)\n",
    "    print(\"GPU-1 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        total, free, used = calc_GPU_info(1)\n",
    "        print(\"GPU-2 Info: Total - {:.2f} GB, Free - {:.2f} GB, Used - {:.2f} GB\".format(bytes_to_giga_bytes(total), bytes_to_giga_bytes(free), bytes_to_giga_bytes(used)))\n",
    "\n",
    "    tokenizer2 = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "    pipe2 = pipeline(\"text-generation\", model=model2, tokenizer=tokenizer2)\n",
    "    \n",
    "    filename = \"input_prompt.txt\"\n",
    "    with open(filename) as file:\n",
    "        for prompt in file:\n",
    "            conv_list = []\n",
    "            lat_list = []\n",
    "            thrput_list = []\n",
    "            for j in range(3):   #Take average and std\n",
    "\n",
    "                conv, lat, thrput = evaluate(pipe2, prompt)\n",
    "                conv_list.append(conv)\n",
    "                lat_list.append(lat)\n",
    "                thrput_list.append(thrput)\n",
    "                \n",
    "            timestamp = time.time()\n",
    "\n",
    "            # Convert the timestamp to a datetime object\n",
    "            dt_object = datetime.fromtimestamp(timestamp)\n",
    "\n",
    "            # Format the datetime object\n",
    "            formatted_time = dt_object.strftime('%Y/%m/%d %H:%M:%S.%f')\n",
    "\n",
    "            print(prompt)\n",
    "            \n",
    "            out_list.append([formatted_time,i,prompt,conv_list,np.mean(lat_list),np.std(lat_list),np.mean(thrput_list),np.std(thrput_list)])\n",
    "            \n",
    "        del model2\n",
    "        flush()\n",
    "        os.kill(pid, signal.SIGINT)\n",
    "        print(f\"======================END=={MODEL}_{i}=========================\")\n",
    "\n",
    "    with open(output_filename, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        csvwriter.writerow(fields)   \n",
    "        csvwriter.writerows(out_list)\n",
    "\n",
    "def multi_evalute_multi_model(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID):\n",
    "    t1 = threading.Thread(target=multi_evalute_multi_model_main, args=(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID,), name='t1')\n",
    "    t2 = threading.Thread(target=multi_evalute_multi_model_small, args=(f\"result_small_{MODEL}_{SELECTION}_{ID}.csv\", MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID,), name='t2')\n",
    "    multi_evalute_multi_model_small(f\"result_small_{MODEL}_{SELECTION}_{ID}.csv\", MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type,ID)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    " \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "\n",
    "print(\"======================START==OF==EXPERIMENT=========================\")\n",
    "\n",
    "MODEL = 'GPT-J' # model for the experiment\n",
    "SELECTION = 1 # selction for mapping\n",
    "ID = 2\n",
    "print(f'Model selected for experiment: {MODEL}')\n",
    "print(f'Device Mapinng selected for experiment: {SELECTION}')\n",
    "\n",
    "# Initialization \n",
    "print(f'Is Cuda Availabe? : {torch.cuda.is_available()}')\n",
    "print(f'Cuda Device Count : {torch.cuda.device_count()}')\n",
    "model_name, model_type, tranformer_type = init_experiment(MODEL) #initialize model and transformer wrappers\n",
    "print(f'Full Name of Model selected for experiment: {model_name}')\n",
    "\n",
    "# Intialise model on memory\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.bfloat16)\n",
    "model2.to('cpu')\n",
    "model_disk = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model_disk.to('cpu')\n",
    "print(f'\\nModel Overview:\\n{model_disk}\\n')\n",
    "print(f'\\nModel Device Initial Mapping:\\n{infer_auto_device_map(model_disk)}\\n')\n",
    "output_filename = f\"result_{MODEL}_{SELECTION}.csv\"\n",
    "#output_filename = f\"result_dual_seq_{MODEL}_{SELECTION}_{ID}.csv\"\n",
    "print(\"======================START==OF==EVALUATION=========================\")\n",
    "#multi_evalute_multi_model(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type ,ID)\n",
    "multi_evalute(output_filename, MODEL, SELECTION, model_disk, model_name, model_type, tranformer_type)\n",
    "print(\"======================END==OF==EVALUATION=========================\")\n",
    "\n",
    "print(\"======================END==OF==EXPERIMENT=========================\")\n",
    "\n",
    "del model_disk\n",
    "flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c813899-dabb-478a-86cd-14df521a31e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
